{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "import re, logging, warnings\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess, lemmatize\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv(\"../data/show_text_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LDA Run with subset of data\n",
    "borrowed liberally from https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = text.sample(frac=.05, random_state=231112334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>show_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4505</th>\n",
       "      <td>1 [siren wailing in distance.]  [helicopter wh...</td>\n",
       "      <td>Westside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>[collective chatter.]  - This is my family  An...</td>\n",
       "      <td>Freakshow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>Miss Sara! Come back here! Slow down!  Buying ...</td>\n",
       "      <td>Avonlea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4307</th>\n",
       "      <td>1 - [WATER DRIPPING.] - [BALL THUMPING.]   [OM...</td>\n",
       "      <td>Treadstone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>1      Here. It's water.  I was gonna make tea...</td>\n",
       "      <td>Unbelievable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    show_names\n",
       "4505  1 [siren wailing in distance.]  [helicopter wh...      Westside\n",
       "1346  [collective chatter.]  - This is my family  An...     Freakshow\n",
       "293   Miss Sara! Come back here! Slow down!  Buying ...       Avonlea\n",
       "4307  1 - [WATER DRIPPING.] - [BALL THUMPING.]   [OM...    Treadstone\n",
       "4372  1      Here. It's water.  I was gonna make tea...  Unbelievable"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(233, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15763371"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([len(show) for show in text[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_chars = []\n",
    "for show in test_df[\"text\"]:\n",
    "    show_chars.append(len(show))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6643592,\n",
       " 5475084,\n",
       " 6388504,\n",
       " 7120597,\n",
       " 6432342,\n",
       " 5056114,\n",
       " 6654870,\n",
       " 5570705,\n",
       " 6140003,\n",
       " 10475318,\n",
       " 5375618,\n",
       " 5995790,\n",
       " 9091967,\n",
       " 6144160,\n",
       " 14341838,\n",
       " 5774170,\n",
       " 13358491,\n",
       " 8246479,\n",
       " 5079156,\n",
       " 7283785,\n",
       " 8661389,\n",
       " 7131747]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[show for show in show_chars if show > 5_000_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Stop Words from SpaCy and nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing stop words from spacy and nltk\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "spacy_stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "nltk_stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanford_file = open(\"../data/stanford_stopwords.txt\", \"r\")\n",
    "stanford_stopwords = stanford_file.read()\n",
    "\n",
    "stanford_stopwords = stanford_stopwords.replace(\"\\n\", \",\").split(\",\")\n",
    "\n",
    "stanford_stopwords = set(stanford_stopwords)\n",
    "\n",
    "stop_words = spacy_stop_words.union(stanford_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stop_words.union(nltk_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "489"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../data/ranks_nl_stopwords.txt\", \"r\")\n",
    "ranks_nl_stopwords = f.read()\n",
    "\n",
    "ranks_nl_stopwords = ranks_nl_stopwords.replace(\"\\n\", \",\").replace(\"\\t\", \",\").split(\",\")[1:]\n",
    "\n",
    "ranks_nl_stopwords = set(ranks_nl_stopwords)\n",
    "\n",
    "stop_words = stop_words.union(ranks_nl_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../data/custom_stopwords.txt\", \"r\")\n",
    "custom_stop_words = f.read()\n",
    "\n",
    "custom_stop_words = set(custom_stop_words.split(\",\"))\n",
    "\n",
    "stop_words = stop_words.union(custom_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1821"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diff_words = []\n",
    "#for word in spacy_stop_words:\n",
    "#    if word not in nltk_stop_words:\n",
    "#        diff_words.append(word)\n",
    "#diff_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are more words in the spacy stop words list will try that one first then will try the nltk list\n",
    "#### And now time to tokenize the text\n",
    "setting deacc=True in order to remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    for word in text:\n",
    "        yield(gensim.utils.simple_preprocess(str(word), deacc=True))\n",
    "\n",
    "test_df[\"text_tokenized\"] = list(tokenizer(test_df[\"text\"]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>show_names</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>1      Here. It's water.  I was gonna make tea...</td>\n",
       "      <td>Unbelievable</td>\n",
       "      <td>[here, it, water, was, gonna, make, tea, but, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>It was the empire on which the sun never set, ...</td>\n",
       "      <td>Empire</td>\n",
       "      <td>[it, was, the, empire, on, which, the, sun, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3933</th>\n",
       "      <td>Oh, honey, are you gorgeous! You look just lik...</td>\n",
       "      <td>The Nanny</td>\n",
       "      <td>[oh, honey, are, you, gorgeous, you, look, jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>1 [Lively jazz music plays.]    Stephen King's...</td>\n",
       "      <td>Bag of Bones</td>\n",
       "      <td>[lively, jazz, music, plays, stephen, king, ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>1 Watch out, Tomoya-kun!  Are you all right, T...</td>\n",
       "      <td>Clannad: After Story</td>\n",
       "      <td>[watch, out, tomoya, kun, are, you, all, right...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text            show_names  \\\n",
       "4372  1      Here. It's water.  I was gonna make tea...          Unbelievable   \n",
       "1183  It was the empire on which the sun never set, ...                Empire   \n",
       "3933  Oh, honey, are you gorgeous! You look just lik...             The Nanny   \n",
       "354   1 [Lively jazz music plays.]    Stephen King's...          Bag of Bones   \n",
       "764   1 Watch out, Tomoya-kun!  Are you all right, T...  Clannad: After Story   \n",
       "\n",
       "                                         text_tokenized  \n",
       "4372  [here, it, water, was, gonna, make, tea, but, ...  \n",
       "1183  [it, was, the, empire, on, which, the, sun, ne...  \n",
       "3933  [oh, honey, are, you, gorgeous, you, look, jus...  \n",
       "354   [lively, jazz, music, plays, stephen, king, ba...  \n",
       "764   [watch, out, tomoya, kun, are, you, all, right...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create bigrams and trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(test_df[\"text_tokenized\"], min_count=10, threshold=500)\n",
    "#trigrams are madde by applying the same method to the bigram output that made the bigrams from the te\n",
    "#trigram = gensim.models.Phrases(bigram[test_df[\"text_tokenized\"]], threshold=500)\n",
    "\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "#trigram_model = gensim.models.phrases.Phraser(trigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define functions to remove stopwords, make bigrams, trigrams, then lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in text]\n",
    "\n",
    "def make_bigrams(text):\n",
    "    return [bigram_model[doc] for doc in text]\n",
    "\n",
    "def make_trigrams(text):\n",
    "    return [trigram_model[bigram_mod[doc]] for doc in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    text_out = []\n",
    "    for sent in text:\n",
    "        doc = nlp(\" \".join(sent), disable=[\"ner\", \"parser\"]) \n",
    "        text_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"text_no_stopwords\"] = remove_stopwords(test_df[\"text_tokenized\"])\n",
    "#test_df[\"text_no_stopwords\"] = remove_stopwords(test_df[\"text_tokenized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_bigrams = make_bigrams(text_no_stopwords)\n",
    "#test_df[\"text_bigrams\"] = make_bigrams(test_df[\"text_no_stopwords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 2_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multithread_lemmatizing(text, threads=3):\n",
    "    pool = Pool(threads)\n",
    "    results = pool.map(lemmatization, text)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_lemmatized = multithread_lemmatizing(text_bigrams)\n",
    "#lemmatized_output = []\n",
    "#for text in text_lemmatized:\n",
    "#    lemmatized_output.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"text_lemmatized\"] = lemmatization(text_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "#options for lemmatization , allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_words(col):\n",
    "    word_count = 0\n",
    "    for row in col:\n",
    "        word_count += len(row)\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1211705"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words(text_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>show_names</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>1      Here. It's water.  I was gonna make tea...</td>\n",
       "      <td>Unbelievable</td>\n",
       "      <td>[here, it, water, was, gonna, make, tea, but, ...</td>\n",
       "      <td>[water, gonna, tea, honey, sip, help, judith, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>It was the empire on which the sun never set, ...</td>\n",
       "      <td>Empire</td>\n",
       "      <td>[it, was, the, empire, on, which, the, sun, ne...</td>\n",
       "      <td>[empire, sun, set, blood, dried, height, brita...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3933</th>\n",
       "      <td>Oh, honey, are you gorgeous! You look just lik...</td>\n",
       "      <td>The Nanny</td>\n",
       "      <td>[oh, honey, are, you, gorgeous, you, look, jus...</td>\n",
       "      <td>[honey, gorgeous, virgin, brought, crackers, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>1 [Lively jazz music plays.]    Stephen King's...</td>\n",
       "      <td>Bag of Bones</td>\n",
       "      <td>[lively, jazz, music, plays, stephen, king, ba...</td>\n",
       "      <td>[lively, jazz, music, plays, stephen, king, ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>1 Watch out, Tomoya-kun!  Are you all right, T...</td>\n",
       "      <td>Clannad: After Story</td>\n",
       "      <td>[watch, out, tomoya, kun, are, you, all, right...</td>\n",
       "      <td>[watch, tomoya, kun, tomoya, kun, yeah, spaced...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text            show_names  \\\n",
       "4372  1      Here. It's water.  I was gonna make tea...          Unbelievable   \n",
       "1183  It was the empire on which the sun never set, ...                Empire   \n",
       "3933  Oh, honey, are you gorgeous! You look just lik...             The Nanny   \n",
       "354   1 [Lively jazz music plays.]    Stephen King's...          Bag of Bones   \n",
       "764   1 Watch out, Tomoya-kun!  Are you all right, T...  Clannad: After Story   \n",
       "\n",
       "                                         text_tokenized  \\\n",
       "4372  [here, it, water, was, gonna, make, tea, but, ...   \n",
       "1183  [it, was, the, empire, on, which, the, sun, ne...   \n",
       "3933  [oh, honey, are, you, gorgeous, you, look, jus...   \n",
       "354   [lively, jazz, music, plays, stephen, king, ba...   \n",
       "764   [watch, out, tomoya, kun, are, you, all, right...   \n",
       "\n",
       "                                      text_no_stopwords  \n",
       "4372  [water, gonna, tea, honey, sip, help, judith, ...  \n",
       "1183  [empire, sun, set, blood, dried, height, brita...  \n",
       "3933  [honey, gorgeous, virgin, brought, crackers, m...  \n",
       "354   [lively, jazz, music, plays, stephen, king, ba...  \n",
       "764   [watch, tomoya, kun, tomoya, kun, yeah, spaced...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary\n",
    "id2word = corpora.Dictionary(test_df[\"text_no_stopwords\"])\n",
    "\n",
    "#the lemmatized text\n",
    "texts = test_df[\"text_no_stopwords\"]\n",
    "\n",
    "#term doc frequency (corpus)\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('aa', 3),\n",
       "  ('abc', 1),\n",
       "  ('ability', 2),\n",
       "  ('abouta', 1),\n",
       "  ('aboutyour', 1),\n",
       "  ('absolutely', 7),\n",
       "  ('abstinent', 1),\n",
       "  ('accept', 1),\n",
       "  ('acceptance', 1),\n",
       "  ('accepted', 1),\n",
       "  ('accomplish', 3),\n",
       "  ('accomplishable', 1),\n",
       "  ('accomplishments', 1),\n",
       "  ('accountable', 1),\n",
       "  ('achieve', 2),\n",
       "  ('acid', 2),\n",
       "  ('acknowledge', 1),\n",
       "  ('acoustic', 1),\n",
       "  ('acting', 13),\n",
       "  ('action', 1),\n",
       "  ('actions', 2),\n",
       "  ('actor', 1),\n",
       "  ('actress', 2),\n",
       "  ('actual', 2),\n",
       "  ('add', 1),\n",
       "  ('addled', 2),\n",
       "  ('addressing', 1),\n",
       "  ('adhd', 1),\n",
       "  ('admit', 1),\n",
       "  ('ado', 4),\n",
       "  ('adores', 1),\n",
       "  ('advice', 2),\n",
       "  ('affect', 2),\n",
       "  ('affection', 2),\n",
       "  ('afford', 2),\n",
       "  ('afraid', 7),\n",
       "  ('age', 4),\n",
       "  ('agenda', 1),\n",
       "  ('agree', 8),\n",
       "  ('ahead', 3),\n",
       "  ('aim', 1),\n",
       "  ('aimlessly', 1),\n",
       "  ('aina', 2),\n",
       "  ('air', 1),\n",
       "  ('airport', 3),\n",
       "  ('album', 1),\n",
       "  ('alcohol', 2),\n",
       "  ('alcoholic', 4),\n",
       "  ('alcoholics', 2),\n",
       "  ('alexander', 1),\n",
       "  ('alexandra', 103),\n",
       "  ('alienate', 1),\n",
       "  ('alike', 2),\n",
       "  ('alive', 8),\n",
       "  ('alley', 1),\n",
       "  ('allow', 1),\n",
       "  ('allowed', 5),\n",
       "  ('allowing', 1),\n",
       "  ('allows', 2),\n",
       "  ('alts', 1),\n",
       "  ('alwaysa', 1),\n",
       "  ('amen', 1),\n",
       "  ('america', 1),\n",
       "  ('american', 6),\n",
       "  ('ana', 3),\n",
       "  ('anchor', 1),\n",
       "  ('anda', 1),\n",
       "  ('angels', 1),\n",
       "  ('anger', 1),\n",
       "  ('angle', 1),\n",
       "  ('angles', 1),\n",
       "  ('angry', 11),\n",
       "  ('animal', 2),\n",
       "  ('annoyed', 2),\n",
       "  ('annoying', 1),\n",
       "  ('anonymous', 1),\n",
       "  ('answer', 1),\n",
       "  ('answered', 1),\n",
       "  ('answers', 2),\n",
       "  ('antidote', 1),\n",
       "  ('anxiety', 5),\n",
       "  ('anxious', 3),\n",
       "  ('anytime', 1),\n",
       "  ('apart', 4),\n",
       "  ('apartment', 1),\n",
       "  ('apologize', 1),\n",
       "  ('apparel', 2),\n",
       "  ('apparent', 2),\n",
       "  ('appealing', 1),\n",
       "  ('appears', 1),\n",
       "  ('applaud', 2),\n",
       "  ('applauding', 4),\n",
       "  ('applauds', 3),\n",
       "  ('applause', 26),\n",
       "  ('apple', 1),\n",
       "  ('apply', 1),\n",
       "  ('appreciate', 5),\n",
       "  ('appreciated', 1),\n",
       "  ('appreciating', 1),\n",
       "  ('apprehension', 1),\n",
       "  ('approval', 1),\n",
       "  ('area', 4),\n",
       "  ('argentina', 1),\n",
       "  ('argument', 1),\n",
       "  ('arika', 60),\n",
       "  ('arizona', 2),\n",
       "  ('armor', 3),\n",
       "  ('art', 11),\n",
       "  ('artist', 13),\n",
       "  ('artistic', 2),\n",
       "  ('artistically', 1),\n",
       "  ('artistry', 4),\n",
       "  ('artists', 7),\n",
       "  ('arvind', 6),\n",
       "  ('ary', 14),\n",
       "  ('asap', 1),\n",
       "  ('ashamed', 1),\n",
       "  ('ashes', 3),\n",
       "  ('asian', 1),\n",
       "  ('asks', 1),\n",
       "  ('asparagus', 1),\n",
       "  ('aspects', 1),\n",
       "  ('ass', 2),\n",
       "  ('asshole', 5),\n",
       "  ('assholes', 1),\n",
       "  ('assignment', 3),\n",
       "  ('assume', 1),\n",
       "  ('atlas', 1),\n",
       "  ('attached', 2),\n",
       "  ('attacked', 1),\n",
       "  ('attacking', 2),\n",
       "  ('attention', 12),\n",
       "  ('attitude', 6),\n",
       "  ('audience', 4),\n",
       "  ('audition', 3),\n",
       "  ('auditions', 2),\n",
       "  ('aunt', 1),\n",
       "  ('authenticity', 3),\n",
       "  ('authoritarian', 1),\n",
       "  ('authority', 1),\n",
       "  ('avatar', 10),\n",
       "  ('aw', 1),\n",
       "  ('awesome', 12),\n",
       "  ('awful', 2),\n",
       "  ('axl', 1),\n",
       "  ('babbles', 1),\n",
       "  ('babe', 9),\n",
       "  ('babies', 2),\n",
       "  ('background', 4),\n",
       "  ('backgrounds', 2),\n",
       "  ('backing', 2),\n",
       "  ('backs', 1),\n",
       "  ('backside', 2),\n",
       "  ('backup', 2),\n",
       "  ('backyard', 1),\n",
       "  ('badass', 5),\n",
       "  ('baddest', 1),\n",
       "  ('badly', 1),\n",
       "  ('bag', 1),\n",
       "  ('bajillions', 1),\n",
       "  ('bake', 1),\n",
       "  ('balance', 5),\n",
       "  ('balding', 2),\n",
       "  ('ball', 2),\n",
       "  ('ballad', 2),\n",
       "  ('balling', 1),\n",
       "  ('balls', 2),\n",
       "  ('band', 22),\n",
       "  ('bands', 4),\n",
       "  ('banger', 1),\n",
       "  ('bank', 1),\n",
       "  ('bar', 1),\n",
       "  ('barbecued', 2),\n",
       "  ('barely', 1),\n",
       "  ('barks', 2),\n",
       "  ('barrier', 1),\n",
       "  ('bars', 2),\n",
       "  ('base', 1),\n",
       "  ('based', 7),\n",
       "  ('basement', 1),\n",
       "  ('bases', 2),\n",
       "  ('basically', 5),\n",
       "  ('basis', 1),\n",
       "  ('basmati', 1),\n",
       "  ('bass', 2),\n",
       "  ('bathroom', 1),\n",
       "  ('batson', 3),\n",
       "  ('battle', 1),\n",
       "  ('battled', 1),\n",
       "  ('bay', 1),\n",
       "  ('bea', 1),\n",
       "  ('bead', 1),\n",
       "  ('beat', 5),\n",
       "  ('beatboxing', 1),\n",
       "  ('beating', 3),\n",
       "  ('beautiful', 47),\n",
       "  ('beautifully', 1),\n",
       "  ('beauty', 17),\n",
       "  ('bed', 2),\n",
       "  ('beep', 1),\n",
       "  ('beeps', 1),\n",
       "  ('beer', 11),\n",
       "  ('begging', 1),\n",
       "  ('beginner', 2),\n",
       "  ('begs', 1),\n",
       "  ('behavior', 4),\n",
       "  ('beinga', 1),\n",
       "  ('belief', 1),\n",
       "  ('believed', 1),\n",
       "  ('believer', 1),\n",
       "  ('believing', 1),\n",
       "  ('bell', 2),\n",
       "  ('belong', 2),\n",
       "  ('belongs', 1),\n",
       "  ('beneath', 1),\n",
       "  ('benefit', 1),\n",
       "  ('bent', 1),\n",
       "  ('besta', 1),\n",
       "  ('betray', 1),\n",
       "  ('bets', 1),\n",
       "  ('beverley', 1),\n",
       "  ('beverly', 3),\n",
       "  ('biased', 2),\n",
       "  ('bigger', 3),\n",
       "  ('biggest', 7),\n",
       "  ('billboard', 1),\n",
       "  ('billion', 2),\n",
       "  ('bills', 2),\n",
       "  ('bimbo', 1),\n",
       "  ('binge', 1),\n",
       "  ('biological', 1),\n",
       "  ('birds', 2),\n",
       "  ('birthday', 3),\n",
       "  ('birthed', 1),\n",
       "  ('bisexuals', 1),\n",
       "  ('bitch', 9),\n",
       "  ('bitches', 1),\n",
       "  ('bitsy', 1),\n",
       "  ('black', 1),\n",
       "  ('blacked', 1),\n",
       "  ('blades', 2),\n",
       "  ('blame', 1),\n",
       "  ('blaring', 2),\n",
       "  ('blatantly', 1),\n",
       "  ('bleed', 2),\n",
       "  ('bless', 2),\n",
       "  ('blessing', 2),\n",
       "  ('blessings', 1),\n",
       "  ('blinged', 1),\n",
       "  ('blink', 1),\n",
       "  ('blithering', 1),\n",
       "  ('block', 1),\n",
       "  ('blocked', 1),\n",
       "  ('blood', 1),\n",
       "  ('blow', 3),\n",
       "  ('blowingly', 1),\n",
       "  ('blown', 2),\n",
       "  ('blows', 1),\n",
       "  ('blueprint', 1),\n",
       "  ('blurred', 1),\n",
       "  ('bmw', 2),\n",
       "  ('board', 4),\n",
       "  ('boasting', 1),\n",
       "  ('bond', 2),\n",
       "  ('book', 4),\n",
       "  ('booked', 1),\n",
       "  ('booking', 2),\n",
       "  ('boom', 7),\n",
       "  ('booths', 1),\n",
       "  ('boots', 1),\n",
       "  ('border', 1),\n",
       "  ('born', 4),\n",
       "  ('boss', 1),\n",
       "  ('bother', 1),\n",
       "  ('bottle', 3),\n",
       "  ('bought', 1),\n",
       "  ('boulevard', 3),\n",
       "  ('bound', 1),\n",
       "  ('boundary', 1),\n",
       "  ('bout', 1),\n",
       "  ('bow', 1),\n",
       "  ('bowl', 1),\n",
       "  ('box', 13),\n",
       "  ('boxes', 1),\n",
       "  ('boxing', 1),\n",
       "  ('boyfriend', 2),\n",
       "  ('boyfriends', 1),\n",
       "  ('brain', 3),\n",
       "  ('brand', 4),\n",
       "  ('brat', 2),\n",
       "  ('bravo', 1),\n",
       "  ('break', 10),\n",
       "  ('breaking', 6),\n",
       "  ('breaks', 5),\n",
       "  ('breath', 2),\n",
       "  ('breathes', 1),\n",
       "  ('bridge', 1),\n",
       "  ('brigham', 2),\n",
       "  ('bright', 2),\n",
       "  ('brights', 1),\n",
       "  ('brim', 1),\n",
       "  ('bringam', 1),\n",
       "  ('bringing', 5),\n",
       "  ('brings', 1),\n",
       "  ('britney', 1),\n",
       "  ('broke', 7),\n",
       "  ('broken', 13),\n",
       "  ('brooklyn', 2),\n",
       "  ('brought', 5),\n",
       "  ('brownie', 2),\n",
       "  ('brushed', 1),\n",
       "  ('bubbles', 1),\n",
       "  ('buddy', 2),\n",
       "  ('budget', 1),\n",
       "  ('build', 1),\n",
       "  ('built', 1),\n",
       "  ('bullshit', 11),\n",
       "  ('bummer', 1),\n",
       "  ('bumps', 1),\n",
       "  ('bunch', 10),\n",
       "  ('burn', 8),\n",
       "  ('burned', 2),\n",
       "  ('burning', 4),\n",
       "  ('burns', 2),\n",
       "  ('burst', 1),\n",
       "  ('bush', 2),\n",
       "  ('busking', 3),\n",
       "  ('busy', 1),\n",
       "  ('buy', 7),\n",
       "  ('buzzing', 1),\n",
       "  ('byous', 2),\n",
       "  ('cage', 1),\n",
       "  ('cait', 7),\n",
       "  ('caitlin', 75),\n",
       "  ('cake', 1),\n",
       "  ('calendar', 1),\n",
       "  ('california', 3),\n",
       "  ('calling', 8),\n",
       "  ('calls', 1),\n",
       "  ('calm', 1),\n",
       "  ('camera', 5),\n",
       "  ('cameras', 1),\n",
       "  ('camp', 1),\n",
       "  ('cancel', 1),\n",
       "  ('cancer', 1),\n",
       "  ('candidly', 1),\n",
       "  ('candle', 6),\n",
       "  ('candles', 2),\n",
       "  ('candy', 2),\n",
       "  ('cap', 1),\n",
       "  ('capacity', 1),\n",
       "  ('cape', 1),\n",
       "  ('capella', 1),\n",
       "  ('capitol', 8),\n",
       "  ('cappella', 2),\n",
       "  ('carea', 1),\n",
       "  ('cared', 1),\n",
       "  ('career', 3),\n",
       "  ('careful', 1),\n",
       "  ('cares', 2),\n",
       "  ('caring', 2),\n",
       "  ('carrey', 5),\n",
       "  ('carried', 2),\n",
       "  ('carry', 3),\n",
       "  ('cars', 1),\n",
       "  ('cashed', 1),\n",
       "  ('cassie', 2),\n",
       "  ('cast', 14),\n",
       "  ('catch', 1),\n",
       "  ('catholic', 1),\n",
       "  ('caught', 5),\n",
       "  ('causality', 1),\n",
       "  ('cawing', 1),\n",
       "  ('celebrate', 2),\n",
       "  ('celebrating', 1),\n",
       "  ('celine', 3),\n",
       "  ('cell', 1),\n",
       "  ('cent', 1),\n",
       "  ('center', 3),\n",
       "  ('central', 1),\n",
       "  ('cents', 1),\n",
       "  ('ceremony', 1),\n",
       "  ('chain', 1),\n",
       "  ('chained', 1),\n",
       "  ('challenging', 1),\n",
       "  ('champagne', 5),\n",
       "  ('chance', 5),\n",
       "  ('chances', 2),\n",
       "  ('change', 8),\n",
       "  ('changed', 5),\n",
       "  ('changes', 1),\n",
       "  ('changing', 3),\n",
       "  ('chanky', 1),\n",
       "  ('channel', 2),\n",
       "  ('channels', 2),\n",
       "  ('chapter', 1),\n",
       "  ('character', 3),\n",
       "  ('charge', 2),\n",
       "  ('charging', 1),\n",
       "  ('charity', 1),\n",
       "  ('charts', 1),\n",
       "  ('chase', 1),\n",
       "  ('chaser', 2),\n",
       "  ('chases', 1),\n",
       "  ('chasing', 1),\n",
       "  ('chatter', 12),\n",
       "  ('chattering', 15),\n",
       "  ('cheapest', 1),\n",
       "  ('cheat', 1),\n",
       "  ('check', 6),\n",
       "  ('cheddar', 3),\n",
       "  ('cheek', 1),\n",
       "  ('cheer', 6),\n",
       "  ('cheering', 27),\n",
       "  ('cheers', 13),\n",
       "  ('cheese', 3),\n",
       "  ('cheesy', 4),\n",
       "  ('cherish', 2),\n",
       "  ('chest', 3),\n",
       "  ('chicken', 1),\n",
       "  ('chickens', 1),\n",
       "  ('chill', 3),\n",
       "  ('chills', 1),\n",
       "  ('chime', 1),\n",
       "  ('chips', 1),\n",
       "  ('choice', 2),\n",
       "  ('choking', 1),\n",
       "  ('chord', 3),\n",
       "  ('chords', 2),\n",
       "  ('choreographer', 1),\n",
       "  ('choreography', 2),\n",
       "  ('chorus', 5),\n",
       "  ('chose', 2),\n",
       "  ('chosen', 1),\n",
       "  ('christian', 2),\n",
       "  ('chuckle', 4),\n",
       "  ('chuckling', 5),\n",
       "  ('chugging', 5),\n",
       "  ('chunky', 1),\n",
       "  ('church', 1),\n",
       "  ('cigarette', 2),\n",
       "  ('cipher', 1),\n",
       "  ('circle', 3),\n",
       "  ('circles', 2),\n",
       "  ('circusy', 1),\n",
       "  ('city', 27),\n",
       "  ('clacking', 1),\n",
       "  ('claim', 1),\n",
       "  ('claimed', 3),\n",
       "  ('clapping', 2),\n",
       "  ('claps', 1),\n",
       "  ('clarification', 1),\n",
       "  ('class', 2),\n",
       "  ('clattering', 2),\n",
       "  ('clean', 2),\n",
       "  ('cleaned', 1),\n",
       "  ('cleaners', 1),\n",
       "  ('cleaning', 1),\n",
       "  ('clear', 7),\n",
       "  ('clearing', 1),\n",
       "  ('clearly', 3),\n",
       "  ('clears', 11),\n",
       "  ('clever', 2),\n",
       "  ('clicking', 2),\n",
       "  ('clicks', 8),\n",
       "  ('clientele', 1),\n",
       "  ('climbed', 1),\n",
       "  ('clink', 1),\n",
       "  ('clinks', 1),\n",
       "  ('clock', 4),\n",
       "  ('closed', 1),\n",
       "  ('closer', 4),\n",
       "  ('closes', 2),\n",
       "  ('closest', 1),\n",
       "  ('closet', 1),\n",
       "  ('closure', 2),\n",
       "  ('clothes', 4),\n",
       "  ('clothing', 2),\n",
       "  ('cloud', 1),\n",
       "  ('clouds', 1),\n",
       "  ('club', 2),\n",
       "  ('coach', 1),\n",
       "  ('coal', 1),\n",
       "  ('coast', 1),\n",
       "  ('coaster', 3),\n",
       "  ('coat', 1),\n",
       "  ('cock', 1),\n",
       "  ('cocks', 1),\n",
       "  ('cocky', 1),\n",
       "  ('coddled', 1),\n",
       "  ('coffee', 2),\n",
       "  ('coffees', 1),\n",
       "  ('coincidentally', 1),\n",
       "  ('colby', 1),\n",
       "  ('collaborate', 2),\n",
       "  ('collaborating', 1),\n",
       "  ('collaboration', 1),\n",
       "  ('collaborative', 1),\n",
       "  ('colliding', 1),\n",
       "  ('combining', 1),\n",
       "  ('comedian', 1),\n",
       "  ('comfort', 5),\n",
       "  ('comfortable', 14),\n",
       "  ('commend', 1),\n",
       "  ('comment', 2),\n",
       "  ('commenting', 1),\n",
       "  ('comments', 1),\n",
       "  ('commerce', 3),\n",
       "  ('commercial', 3),\n",
       "  ('commit', 2),\n",
       "  ('commitment', 3),\n",
       "  ('committed', 1),\n",
       "  ('community', 3),\n",
       "  ('company', 2),\n",
       "  ('compare', 3),\n",
       "  ('compelling', 1),\n",
       "  ('competing', 1),\n",
       "  ('competition', 5),\n",
       "  ('complained', 1),\n",
       "  ('complement', 1),\n",
       "  ('completed', 1),\n",
       "  ('completely', 6),\n",
       "  ('complicated', 3),\n",
       "  ('computers', 1),\n",
       "  ('conceived', 2),\n",
       "  ('concentrate', 1),\n",
       "  ('concept', 8),\n",
       "  ('concerned', 4),\n",
       "  ('concert', 1),\n",
       "  ('concerts', 1),\n",
       "  ('concludes', 1),\n",
       "  ('conduit', 1),\n",
       "  ('confidence', 4),\n",
       "  ('confident', 4),\n",
       "  ('confirm', 1),\n",
       "  ('confront', 1),\n",
       "  ('confronted', 1),\n",
       "  ('confused', 2),\n",
       "  ('congratulations', 1),\n",
       "  ('connect', 3),\n",
       "  ('connecting', 1),\n",
       "  ('connections', 2),\n",
       "  ('consensus', 1),\n",
       "  ('consequences', 1),\n",
       "  ('consistent', 1),\n",
       "  ('consistently', 1),\n",
       "  ('constant', 4),\n",
       "  ('constantly', 1),\n",
       "  ('construction', 1),\n",
       "  ('constructive', 2),\n",
       "  ('consumed', 1),\n",
       "  ('contagious', 1),\n",
       "  ('contained', 1),\n",
       "  ('content', 3),\n",
       "  ('continually', 1),\n",
       "  ('continue', 5),\n",
       "  ('continues', 14),\n",
       "  ('continuous', 1),\n",
       "  ('contradicting', 1),\n",
       "  ('conversation', 12),\n",
       "  ('convince', 1),\n",
       "  ('cook', 5),\n",
       "  ('cooking', 2),\n",
       "  ('cooperative', 1),\n",
       "  ('coordinate', 1),\n",
       "  ('cop', 7),\n",
       "  ('cops', 1),\n",
       "  ('copy', 2),\n",
       "  ('corn', 1),\n",
       "  ('corner', 5),\n",
       "  ('correct', 1),\n",
       "  ('cosmos', 1),\n",
       "  ('costs', 1),\n",
       "  ('coulda', 1),\n",
       "  ('count', 2),\n",
       "  ('counting', 3),\n",
       "  ('country', 6),\n",
       "  ('couple', 17),\n",
       "  ('couplea', 1),\n",
       "  ('cover', 1),\n",
       "  ('covered', 3),\n",
       "  ('covers', 5),\n",
       "  ('cow', 1),\n",
       "  ('coward', 1),\n",
       "  ('cowrite', 1),\n",
       "  ('cows', 1),\n",
       "  ('cozy', 1),\n",
       "  ('crack', 2),\n",
       "  ('cracked', 1),\n",
       "  ('cracking', 1),\n",
       "  ('cracks', 1),\n",
       "  ('craft', 1),\n",
       "  ('crank', 1),\n",
       "  ('crap', 1),\n",
       "  ('crashed', 1),\n",
       "  ('craving', 1),\n",
       "  ('crazy', 24),\n",
       "  ('cream', 1),\n",
       "  ('create', 8),\n",
       "  ('created', 10),\n",
       "  ('creates', 1),\n",
       "  ('creating', 8),\n",
       "  ('creative', 13),\n",
       "  ('creativity', 3),\n",
       "  ('creators', 1),\n",
       "  ('crickets', 1),\n",
       "  ('cried', 1),\n",
       "  ('cries', 9),\n",
       "  ('crime', 3),\n",
       "  ('crisis', 1),\n",
       "  ('critical', 1),\n",
       "  ('criticism', 3),\n",
       "  ('critiqued', 1),\n",
       "  ('crop', 1),\n",
       "  ('cross', 1),\n",
       "  ('crossed', 4),\n",
       "  ('crow', 1),\n",
       "  ('crowd', 8),\n",
       "  ('crowds', 1),\n",
       "  ('cruise', 1),\n",
       "  ('crumbling', 1),\n",
       "  ('crush', 2),\n",
       "  ('cry', 10),\n",
       "  ('crying', 12),\n",
       "  ('cultivated', 1),\n",
       "  ('culture', 1),\n",
       "  ('cup', 1),\n",
       "  ('cupboard', 6),\n",
       "  ('cups', 1),\n",
       "  ('curb', 1),\n",
       "  ('cure', 1),\n",
       "  ('current', 1),\n",
       "  ('curse', 3),\n",
       "  ('custom', 2),\n",
       "  ('cute', 4),\n",
       "  ('cutea', 1),\n",
       "  ('cuter', 1),\n",
       "  ('cw', 1),\n",
       "  ('cycle', 1),\n",
       "  ('dah', 9),\n",
       "  ('dakota', 1),\n",
       "  ('damage', 6),\n",
       "  ('damned', 4),\n",
       "  ('dance', 11),\n",
       "  ('danced', 1),\n",
       "  ('dancer', 3),\n",
       "  ('dances', 1),\n",
       "  ('dancing', 6),\n",
       "  ('dang', 2),\n",
       "  ('danger', 1),\n",
       "  ('dangerous', 2),\n",
       "  ('dark', 3),\n",
       "  ('darker', 1),\n",
       "  ('darkest', 1),\n",
       "  ('darkness', 5),\n",
       "  ('data', 1),\n",
       "  ('dating', 3),\n",
       "  ('daughter', 1),\n",
       "  ('dawg', 1),\n",
       "  ('daylight', 1),\n",
       "  ('dazzling', 1),\n",
       "  ('deadlines', 1),\n",
       "  ('dealing', 2),\n",
       "  ('deals', 1),\n",
       "  ('dearth', 1),\n",
       "  ('death', 4),\n",
       "  ('december', 1),\n",
       "  ('decent', 1),\n",
       "  ('decide', 4),\n",
       "  ('decided', 2),\n",
       "  ('decides', 1),\n",
       "  ('decision', 2),\n",
       "  ('decisions', 1),\n",
       "  ('deck', 1),\n",
       "  ('deep', 4),\n",
       "  ('deeper', 1),\n",
       "  ('deeply', 20),\n",
       "  ('def', 1),\n",
       "  ('defeated', 1),\n",
       "  ('defend', 1),\n",
       "  ('defense', 1),\n",
       "  ('defensiveness', 1),\n",
       "  ('define', 1),\n",
       "  ('defining', 2),\n",
       "  ('definitely', 12),\n",
       "  ('deflecting', 1),\n",
       "  ('deforrest', 3),\n",
       "  ('degrees', 1),\n",
       "  ('deh', 1),\n",
       "  ('delicious', 1),\n",
       "  ('deliver', 3),\n",
       "  ('delivering', 1),\n",
       "  ('delusion', 1),\n",
       "  ('democratic', 2),\n",
       "  ('denise', 4),\n",
       "  ('deny', 1),\n",
       "  ('depending', 2),\n",
       "  ('depression', 1),\n",
       "  ('describe', 4),\n",
       "  ('desert', 1),\n",
       "  ('deserve', 2),\n",
       "  ('deserves', 1),\n",
       "  ('desire', 2),\n",
       "  ('desires', 1),\n",
       "  ('desperation', 1),\n",
       "  ('destination', 1),\n",
       "  ('destroy', 1),\n",
       "  ('destructive', 1),\n",
       "  ('detailed', 1),\n",
       "  ('determine', 1),\n",
       "  ('devastation', 1),\n",
       "  ('developing', 2),\n",
       "  ('development', 2),\n",
       "  ('diamond', 2),\n",
       "  ('diamonds', 2),\n",
       "  ('died', 2),\n",
       "  ('diegoa', 1),\n",
       "  ('difference', 2),\n",
       "  ('differently', 1),\n",
       "  ('difficult', 7),\n",
       "  ('dig', 6),\n",
       "  ('dim', 1),\n",
       "  ('dime', 5),\n",
       "  ('dimensional', 1),\n",
       "  ('dings', 3),\n",
       "  ('dinner', 2),\n",
       "  ('dion', 2),\n",
       "  ('direct', 1),\n",
       "  ('directing', 1),\n",
       "  ('direction', 1),\n",
       "  ('directions', 1),\n",
       "  ('director', 5),\n",
       "  ('dirt', 1),\n",
       "  ('dirty', 1),\n",
       "  ('disagree', 2),\n",
       "  ('disappoint', 1),\n",
       "  ('disappointed', 4),\n",
       "  ('disbelief', 1),\n",
       "  ('discipline', 1),\n",
       "  ('disco', 3),\n",
       "  ('disconnect', 1),\n",
       "  ('discovered', 1),\n",
       "  ('discovering', 2),\n",
       "  ('discussed', 1),\n",
       "  ('discussion', 1),\n",
       "  ('discussions', 1),\n",
       "  ('disease', 1),\n",
       "  ('dismissing', 1),\n",
       "  ('disney', 3),\n",
       "  ('disrespect', 2),\n",
       "  ('disrespectful', 2),\n",
       "  ('dissecting', 1),\n",
       "  ('disservice', 1),\n",
       "  ('dissuade', 1),\n",
       "  ('distance', 21),\n",
       "  ('distant', 1),\n",
       "  ('distractions', 1),\n",
       "  ('ditch', 1),\n",
       "  ('dive', 6),\n",
       "  ('divebar', 3),\n",
       "  ('dj', 1),\n",
       "  ('djed', 1),\n",
       "  ('dna', 1),\n",
       "  ('doa', 1),\n",
       "  ('doctor', 1),\n",
       "  ('dog', 5),\n",
       "  ('dogs', 1),\n",
       "  ('dollar', 3),\n",
       "  ('dollars', 1),\n",
       "  ('doo', 1),\n",
       "  ('doors', 5),\n",
       "  ('dope', 7),\n",
       "  ('dots', 1),\n",
       "  ('double', 2),\n",
       "  ('doubt', 4),\n",
       "  ('doubting', 1),\n",
       "  ('dough', 2),\n",
       "  ('downfall', 1),\n",
       "  ('downs', 1),\n",
       "  ('downstage', 1),\n",
       "  ('downstairs', 1),\n",
       "  ('downtown', 1),\n",
       "  ('drag', 1),\n",
       "  ('dragged', 1),\n",
       "  ('drained', 2),\n",
       "  ('draining', 1),\n",
       "  ('drama', 3),\n",
       "  ('dramatic', 2),\n",
       "  ('drank', 2),\n",
       "  ('drawing', 1),\n",
       "  ('dream', 7),\n",
       "  ('dreamed', 1),\n",
       "  ('dreamer', 3),\n",
       "  ('dreaming', 1),\n",
       "  ('dreams', 14),\n",
       "  ('dress', 4),\n",
       "  ('dressed', 1),\n",
       "  ('dresses', 1),\n",
       "  ('drill', 1),\n",
       "  ('drink', 16),\n",
       "  ('drinking', 15),\n",
       "  ('drinks', 2),\n",
       "  ('drip', 1),\n",
       "  ('dripping', 3),\n",
       "  ('drive', 4),\n",
       "  ('driving', 2),\n",
       "  ('drop', 2),\n",
       "  ('dropout', 3),\n",
       "  ('dropped', 1),\n",
       "  ('drove', 1),\n",
       "  ('drowning', 1),\n",
       "  ('drug', 2),\n",
       "  ('drugs', 6),\n",
       "  ('drums', 4),\n",
       "  ('drunk', 7),\n",
       "  ('dry', 3),\n",
       "  ('du', 3),\n",
       "  ('dudes', 1),\n",
       "  ('duis', 1),\n",
       "  ('dumb', 1),\n",
       "  ('dwell', 1),\n",
       "  ('dying', 3),\n",
       "  ('eagle', 1),\n",
       "  ('earlier', 1),\n",
       "  ('early', 1),\n",
       "  ('ears', 1),\n",
       "  ('earth', 1),\n",
       "  ('easier', 1),\n",
       "  ('east', 1),\n",
       "  ('easy', 9),\n",
       "  ('eat', 4),\n",
       "  ('echoes', 3),\n",
       "  ('echoing', 1),\n",
       "  ('edge', 2),\n",
       "  ('edit', 1),\n",
       "  ('editing', 1),\n",
       "  ('edm', 1),\n",
       "  ('eggs', 1),\n",
       "  ('ego', 1),\n",
       "  ('eighth', 1),\n",
       "  ('electric', 1),\n",
       "  ('electrified', 7),\n",
       "  ('elevated', 1),\n",
       "  ('elevator', 1),\n",
       "  ('eloquent', 1),\n",
       "  ('email', 5),\n",
       "  ('emails', 1),\n",
       "  ('embarrassed', 2),\n",
       "  ('embarrassing', 3),\n",
       "  ('embrace', 1),\n",
       "  ('embracing', 1),\n",
       "  ('emotional', 1),\n",
       "  ('emotionally', 5),\n",
       "  ('emotions', 3),\n",
       "  ('empowering', 4),\n",
       "  ('encompasses', 1),\n",
       "  ('encountered', 1),\n",
       "  ('encourage', 2),\n",
       "  ('encouragement', 1),\n",
       "  ('endearing', 2),\n",
       "  ('ended', 5),\n",
       "  ('enemy', 1),\n",
       "  ('energy', 1),\n",
       "  ('engaged', 1),\n",
       "  ('engine', 6),\n",
       "  ('englewood', 1),\n",
       "  ('enjoy', 4),\n",
       "  ('enjoying', 1),\n",
       "  ('ensemble', 4),\n",
       "  ('enter', 2),\n",
       "  ('entering', 1),\n",
       "  ('entertain', 1),\n",
       "  ('entertained', 1),\n",
       "  ('entertainer', 1),\n",
       "  ('entertaining', 1),\n",
       "  ('entertainment', 1),\n",
       "  ('entire', 7),\n",
       "  ('entourage', 1),\n",
       "  ('enunciate', 1),\n",
       "  ('environment', 1),\n",
       "  ('envisioned', 1),\n",
       "  ('epic', 4),\n",
       "  ('equal', 1),\n",
       "  ('error', 1),\n",
       "  ('escape', 1),\n",
       "  ('essence', 1),\n",
       "  ('essentially', 3),\n",
       "  ('establish', 1),\n",
       "  ('eternity', 2),\n",
       "  ('ether', 1),\n",
       "  ('ethic', 1),\n",
       "  ('evacuate', 6),\n",
       "  ('event', 2),\n",
       "  ('everybodya', 3),\n",
       "  ('everyday', 1),\n",
       "  ('evicted', 1),\n",
       "  ('evil', 4),\n",
       "  ('eviscerating', 1),\n",
       "  ('evolve', 2),\n",
       "  ('exact', 2),\n",
       "  ('example', 1),\n",
       "  ('excel', 1),\n",
       "  ('exceptional', 1),\n",
       "  ('exchange', 1),\n",
       "  ('excited', 20),\n",
       "  ('exciting', 5),\n",
       "  ('excuses', 1),\n",
       "  ('executives', 3),\n",
       "  ('exercise', 8),\n",
       "  ('exercises', 1),\n",
       "  ('exhales', 6),\n",
       "  ('exhausted', 2),\n",
       "  ('exhausting', 1),\n",
       "  ('exist', 4),\n",
       "  ('existence', 1),\n",
       "  ('expect', 2),\n",
       "  ('expectation', 1),\n",
       "  ('expectations', 1),\n",
       "  ('expecting', 1),\n",
       "  ('experience', 10),\n",
       "  ('experienced', 2),\n",
       "  ('experiences', 2),\n",
       "  ('experiencing', 1),\n",
       "  ('expired', 1),\n",
       "  ('exploit', 1),\n",
       "  ('exploited', 2),\n",
       "  ('exploiting', 1),\n",
       "  ('explore', 2),\n",
       "  ('exposed', 3),\n",
       "  ('expressing', 1),\n",
       "  ('extra', 2),\n",
       "  ('extremely', 1),\n",
       "  ('exudes', 1),\n",
       "  ('eye', 4),\n",
       "  ('eyes', 15),\n",
       "  ('fabulous', 1),\n",
       "  ('facade', 3),\n",
       "  ('facebook', 3),\n",
       "  ('faced', 1),\n",
       "  ('facilitate', 1),\n",
       "  ('fact', 8),\n",
       "  ('factory', 1),\n",
       "  ('facts', 1),\n",
       "  ('fail', 7),\n",
       "  ('failed', 6),\n",
       "  ('failing', 6),\n",
       "  ('fails', 2),\n",
       "  ('failure', 3),\n",
       "  ('fair', 1),\n",
       "  ('faith', 1),\n",
       "  ('fake', 3),\n",
       "  ('fall', 6),\n",
       "  ('fallen', 1),\n",
       "  ('falling', 1),\n",
       "  ('falsetto', 1),\n",
       "  ('fame', 2),\n",
       "  ('familiar', 1),\n",
       "  ('famous', 2),\n",
       "  ('fan', 4),\n",
       "  ('fantastic', 1),\n",
       "  ('farm', 1),\n",
       "  ('fart', 2),\n",
       "  ('fascinating', 2),\n",
       "  ('fast', 1),\n",
       "  ('faucet', 2),\n",
       "  ('fault', 5),\n",
       "  ('favor', 1),\n",
       "  ('favorably', 1),\n",
       "  ('fazed', 3),\n",
       "  ('fear', 6),\n",
       "  ('fears', 1),\n",
       "  ('feed', 13),\n",
       "  ('feedback', 1),\n",
       "  ('feeds', 2),\n",
       "  ('feeling', 48),\n",
       "  ('feelings', 4),\n",
       "  ('feels', 8),\n",
       "  ('feet', 2),\n",
       "  ('fell', 1),\n",
       "  ('felt', 22),\n",
       "  ('femininity', 1),\n",
       "  ('festering', 1),\n",
       "  ('fever', 2),\n",
       "  ('fiery', 1),\n",
       "  ('fighter', 1),\n",
       "  ('fighting', 15),\n",
       "  ('figure', 12),\n",
       "  ('figured', 6),\n",
       "  ('figuring', 6),\n",
       "  ('filters', 1),\n",
       "  ('final', 2),\n",
       "  ('finalist', 1),\n",
       "  ('finally', 16),\n",
       "  ('finances', 2),\n",
       "  ('financial', 1),\n",
       "  ('financially', 1),\n",
       "  ('finding', 4),\n",
       "  ('finds', 1),\n",
       "  ('finesse', 1),\n",
       "  ('fingers', 2),\n",
       "  ...]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=15, \n",
    "                                           random_state=12,\n",
    "                                           update_every=10,\n",
    "                                           chunksize=5,\n",
    "                                           passes=5,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.008*\"advice\" + 0.006*\"micki\" + 0.006*\"lewis\" + 0.005*\"lord\" + '\n",
      "  '0.004*\"dark\" + 0.004*\"local\" + 0.004*\"helena\" + 0.004*\"earthquake\" + '\n",
      "  '0.003*\"imagining\" + 0.003*\"station\"'),\n",
      " (1,\n",
      "  '0.003*\"ship\" + 0.003*\"water\" + 0.003*\"earth\" + 0.002*\"fire\" + 0.002*\"lois\" '\n",
      "  '+ 0.002*\"dr\" + 0.002*\"planet\" + 0.002*\"lost\" + 0.002*\"black\" + '\n",
      "  '0.002*\"continues\"'),\n",
      " (2,\n",
      "  '0.004*\"school\" + 0.003*\"money\" + 0.003*\"pretty\" + 0.002*\"friends\" + '\n",
      "  '0.002*\"game\" + 0.002*\"crazy\" + 0.002*\"playing\" + 0.002*\"party\" + '\n",
      "  '0.002*\"eat\" + 0.002*\"hot\"'),\n",
      " (3,\n",
      "  '0.004*\"rosemary\" + 0.002*\"margaux\" + 0.001*\"money\" + 0.001*\"ro\" + '\n",
      "  '0.001*\"frost\" + 0.001*\"nena\" + 0.001*\"julie\" + 0.001*\"police\" + '\n",
      "  '0.001*\"micki\" + 0.001*\"marcato\"'),\n",
      " (4,\n",
      "  '0.004*\"blood\" + 0.003*\"water\" + 0.003*\"heart\" + 0.002*\"food\" + '\n",
      "  '0.002*\"doctor\" + 0.002*\"health\" + 0.002*\"fact\" + 0.002*\"frost\" + '\n",
      "  '0.002*\"brain\" + 0.002*\"eat\"'),\n",
      " (5,\n",
      "  '0.011*\"toast\" + 0.004*\"rock\" + 0.002*\"bloody\" + 0.002*\"purchase\" + '\n",
      "  '0.002*\"ursula\" + 0.002*\"actor\" + 0.002*\"matthias\" + 0.002*\"money\" + '\n",
      "  '0.001*\"police\" + 0.001*\"sex\"'),\n",
      " (6,\n",
      "  '0.004*\"money\" + 0.004*\"bloody\" + 0.003*\"hyacinth\" + 0.002*\"drink\" + '\n",
      "  '0.002*\"susie\" + 0.002*\"party\" + 0.002*\"brett\" + 0.002*\"dog\" + 0.002*\"tea\" + '\n",
      "  '0.002*\"tomorrow\"'),\n",
      " (7,\n",
      "  '0.005*\"police\" + 0.004*\"money\" + 0.003*\"detective\" + 0.003*\"gun\" + '\n",
      "  '0.002*\"murder\" + 0.002*\"shot\" + 0.002*\"check\" + 0.002*\"clark\" + '\n",
      "  '0.002*\"pretty\" + 0.002*\"death\"'),\n",
      " (8,\n",
      "  '0.008*\"chava\" + 0.006*\"cuervos\" + 0.006*\"playing\" + 0.005*\"money\" + '\n",
      "  '0.005*\"isabel\" + 0.005*\"marius\" + 0.003*\"iglesias\" + 0.003*\"toledo\" + '\n",
      "  '0.003*\"nuevo\" + 0.003*\"aitor\"'),\n",
      " (9,\n",
      "  '0.005*\"usui\" + 0.005*\"fanny\" + 0.004*\"school\" + 0.004*\"misa\" + '\n",
      "  '0.004*\"misaki\" + 0.004*\"michelle\" + 0.003*\"penguin\" + 0.003*\"prez\" + '\n",
      "  '0.003*\"penguins\" + 0.003*\"maid\"'),\n",
      " (10,\n",
      "  '0.004*\"steed\" + 0.004*\"king\" + 0.004*\"lord\" + 0.003*\"money\" + 0.003*\"miles\" '\n",
      "  '+ 0.002*\"death\" + 0.002*\"afraid\" + 0.002*\"colby\" + 0.002*\"police\" + '\n",
      "  '0.002*\"peel\"'),\n",
      " (11,\n",
      "  '0.006*\"sang\" + 0.006*\"president\" + 0.004*\"king\" + 0.004*\"laws\" + '\n",
      "  '0.004*\"fiction\" + 0.004*\"organizations\" + 0.004*\"resemblance\" + '\n",
      "  '0.004*\"purely\" + 0.004*\"coincidental\" + 0.003*\"heart\"'),\n",
      " (12,\n",
      "  '0.022*\"gumball\" + 0.005*\"anais\" + 0.004*\"school\" + 0.003*\"screaming\" + '\n",
      "  '0.003*\"ugh\" + 0.003*\"star\" + 0.003*\"dragon\" + 0.003*\"voice\" + 0.003*\"magic\" '\n",
      "  '+ 0.003*\"laughing\"'),\n",
      " (13,\n",
      "  '0.013*\"notch\" + 0.006*\"johnson\" + 0.003*\"constable\" + 0.003*\"habib\" + '\n",
      "  '0.003*\"chip\" + 0.003*\"mayor\" + 0.003*\"kimberlee\" + 0.003*\"goody\" + '\n",
      "  '0.003*\"ty\" + 0.002*\"ton\"'),\n",
      " (14,\n",
      "  '0.007*\"cars\" + 0.006*\"drive\" + 0.005*\"engine\" + 0.004*\"road\" + 0.004*\"fast\" '\n",
      "  '+ 0.004*\"driving\" + 0.004*\"hammond\" + 0.004*\"speed\" + 0.003*\"gear\" + '\n",
      "  '0.003*\"track\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -10.163023786135073\n"
     ]
    }
   ],
   "source": [
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.34493726356979965\n"
     ]
    }
   ],
   "source": [
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=test_df[\"text_no_stopwords\"], dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
