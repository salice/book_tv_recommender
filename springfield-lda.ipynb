{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "import re, logging, warnings\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pd.read_csv(\"../data/show_text_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>show_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can it be the breeze that fills the trees  Wit...</td>\n",
       "      <td>'Allo 'Allo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'd like to marry a girl that's skinny. I thin...</td>\n",
       "      <td>'Til Death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's that dream again. Just who is that guy? W...</td>\n",
       "      <td>07 Ghost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 You all might have top-class credentials fro...</td>\n",
       "      <td>1,000 Places To See Before You Die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1  Welcome to 10 O'Clock Live. It's Wednesday ...</td>\n",
       "      <td>10 O'Clock Live</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Can it be the breeze that fills the trees  Wit...   \n",
       "1  I'd like to marry a girl that's skinny. I thin...   \n",
       "2  It's that dream again. Just who is that guy? W...   \n",
       "3  1 You all might have top-class credentials fro...   \n",
       "4  1  Welcome to 10 O'Clock Live. It's Wednesday ...   \n",
       "\n",
       "                           show_names  \n",
       "0                        'Allo 'Allo!  \n",
       "1                          'Til Death  \n",
       "2                            07 Ghost  \n",
       "3  1,000 Places To See Before You Die  \n",
       "4                     10 O'Clock Live  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LDA Run with subset of data\n",
    "borrowed liberally from https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = text.sample(frac=.25, random_state=740)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Stop Words from SpaCy and nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing stop words from spacy and nltk\n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stop_words = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spacy_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whether',\n",
       " 'much',\n",
       " 'made',\n",
       " 'perhaps',\n",
       " 'last',\n",
       " 'whence',\n",
       " 'wherein',\n",
       " 'would',\n",
       " 'hereby',\n",
       " 'formerly',\n",
       " 'moreover',\n",
       " 'every',\n",
       " 'since',\n",
       " 'first',\n",
       " 'whither',\n",
       " 'make',\n",
       " 'somehow',\n",
       " 'already',\n",
       " 'sixty',\n",
       " 'various',\n",
       " 'latterly',\n",
       " 'sometime',\n",
       " 'keep',\n",
       " 'give',\n",
       " 'go',\n",
       " 'together',\n",
       " 'may',\n",
       " 'still',\n",
       " 'ca',\n",
       " 'across',\n",
       " 'though',\n",
       " 'per',\n",
       " 'many',\n",
       " 'whoever',\n",
       " 'towards',\n",
       " 'n’t',\n",
       " 'serious',\n",
       " 'else',\n",
       " 'eleven',\n",
       " 'three',\n",
       " 'next',\n",
       " 'others',\n",
       " 'used',\n",
       " 'among',\n",
       " 'move',\n",
       " 'also',\n",
       " \"'ll\",\n",
       " 'mostly',\n",
       " \"'ve\",\n",
       " 'twenty',\n",
       " 'somewhere',\n",
       " 'eight',\n",
       " 'fifty',\n",
       " 'thereafter',\n",
       " '‘s',\n",
       " 'cannot',\n",
       " 'even',\n",
       " 'say',\n",
       " 'empty',\n",
       " 'take',\n",
       " \"'d\",\n",
       " 'top',\n",
       " 'amount',\n",
       " '’m',\n",
       " 'anyone',\n",
       " 'someone',\n",
       " '‘ll',\n",
       " 'without',\n",
       " 'hereupon',\n",
       " 'side',\n",
       " '‘ve',\n",
       " '’re',\n",
       " 'otherwise',\n",
       " 'must',\n",
       " 'show',\n",
       " 'afterwards',\n",
       " 'whenever',\n",
       " 'bottom',\n",
       " 'thru',\n",
       " 'front',\n",
       " 'get',\n",
       " 'rather',\n",
       " 'another',\n",
       " 'twelve',\n",
       " 'ten',\n",
       " 'alone',\n",
       " 'although',\n",
       " \"'m\",\n",
       " 'ever',\n",
       " 'see',\n",
       " '‘m',\n",
       " 'please',\n",
       " 'unless',\n",
       " 'everything',\n",
       " 'neither',\n",
       " 'enough',\n",
       " 'five',\n",
       " 'wherever',\n",
       " 'regarding',\n",
       " 'less',\n",
       " 'mine',\n",
       " 'except',\n",
       " 'therefore',\n",
       " 'onto',\n",
       " 'thereby',\n",
       " '’d',\n",
       " 'along',\n",
       " 'anything',\n",
       " 'done',\n",
       " 'upon',\n",
       " 'none',\n",
       " 'third',\n",
       " 'thus',\n",
       " 'thereupon',\n",
       " 'whereas',\n",
       " 'becomes',\n",
       " 'least',\n",
       " 'either',\n",
       " 'seemed',\n",
       " 'hundred',\n",
       " 'seems',\n",
       " 'four',\n",
       " 'something',\n",
       " 'namely',\n",
       " 'us',\n",
       " \"n't\",\n",
       " 'meanwhile',\n",
       " 'besides',\n",
       " 'name',\n",
       " 'put',\n",
       " 'due',\n",
       " 'throughout',\n",
       " 'became',\n",
       " 'whose',\n",
       " 'beforehand',\n",
       " '’ve',\n",
       " 'latter',\n",
       " '‘d',\n",
       " 'back',\n",
       " \"'s\",\n",
       " 'noone',\n",
       " 'forty',\n",
       " 'seem',\n",
       " 'anywhere',\n",
       " 'whatever',\n",
       " 'fifteen',\n",
       " 'using',\n",
       " 'could',\n",
       " 'well',\n",
       " 'whereupon',\n",
       " 'n‘t',\n",
       " 'full',\n",
       " 'sometimes',\n",
       " 'seeming',\n",
       " 'always',\n",
       " 'one',\n",
       " 'within',\n",
       " 'nothing',\n",
       " 'part',\n",
       " 'whereafter',\n",
       " 'hence',\n",
       " 'quite',\n",
       " 'never',\n",
       " 'toward',\n",
       " 'really',\n",
       " 'amongst',\n",
       " 'everywhere',\n",
       " 'anyhow',\n",
       " '‘re',\n",
       " 'call',\n",
       " 'behind',\n",
       " 'become',\n",
       " '’s',\n",
       " 'nowhere',\n",
       " 'however',\n",
       " 'nobody',\n",
       " 'beyond',\n",
       " 'indeed',\n",
       " 'yet',\n",
       " 'several',\n",
       " 'everyone',\n",
       " 'whereby',\n",
       " 'nine',\n",
       " 'via',\n",
       " 'anyway',\n",
       " 'nevertheless',\n",
       " 'elsewhere',\n",
       " 'former',\n",
       " 'whole',\n",
       " 'therein',\n",
       " 'thence',\n",
       " 'hereafter',\n",
       " \"'re\",\n",
       " '’ll',\n",
       " 'becoming',\n",
       " 'herein',\n",
       " 'often',\n",
       " 'almost',\n",
       " 'around',\n",
       " 'two',\n",
       " 'beside',\n",
       " 'six',\n",
       " 'might']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_words = []\n",
    "for word in spacy_stop_words:\n",
    "    if word not in nltk_stop_words:\n",
    "        diff_words.append(word)\n",
    "diff_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are more words in the spacy stop words list will try that one first then will try the nltk list\n",
    "#### And now time to tokenize the text\n",
    "setting deacc=True in order to remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    for word in text:\n",
    "        yield(gensim.utils.simple_preprocess(str(word), deacc=True))\n",
    "\n",
    "test_df[\"text_tokenized\"] = list(tokenizer(test_df[\"text\"]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>show_names</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>How about a beer, chief? How about an ID? An I...</td>\n",
       "      <td>Cheers</td>\n",
       "      <td>[how, about, beer, chief, how, about, an, id, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2160</th>\n",
       "      <td>I am fascinated by last words.  Like Oscar Wil...</td>\n",
       "      <td>Looking for Alaska</td>\n",
       "      <td>[am, fascinated, by, last, words, like, oscar,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3216</th>\n",
       "      <td>1 [Machine-gun fire.]  Keep your heads down! W...</td>\n",
       "      <td>Six</td>\n",
       "      <td>[machine, gun, fire, keep, your, heads, down, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>1 HE SPEAKS OWN LANGUAGE  BONE CRACKS, HE SCRE...</td>\n",
       "      <td>Famalam</td>\n",
       "      <td>[he, speaks, own, language, bone, cracks, he, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>SORRY NO .idx-.sub FILES IN THE OS  There is a...</td>\n",
       "      <td>Hostages</td>\n",
       "      <td>[sorry, no, idx, sub, files, in, the, os, ther...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text          show_names  \\\n",
       "727   How about a beer, chief? How about an ID? An I...              Cheers   \n",
       "2160  I am fascinated by last words.  Like Oscar Wil...  Looking for Alaska   \n",
       "3216  1 [Machine-gun fire.]  Keep your heads down! W...                 Six   \n",
       "1244  1 HE SPEAKS OWN LANGUAGE  BONE CRACKS, HE SCRE...             Famalam   \n",
       "1708  SORRY NO .idx-.sub FILES IN THE OS  There is a...            Hostages   \n",
       "\n",
       "                                         text_tokenized  \n",
       "727   [how, about, beer, chief, how, about, an, id, ...  \n",
       "2160  [am, fascinated, by, last, words, like, oscar,...  \n",
       "3216  [machine, gun, fire, keep, your, heads, down, ...  \n",
       "1244  [he, speaks, own, language, bone, cracks, he, ...  \n",
       "1708  [sorry, no, idx, sub, files, in, the, os, ther...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create bigrams and trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(test_df[\"text_tokenized\"], min_count=5, threshold=50)\n",
    "#trigrams are madde by applying the same method to the bigram output that made the bigrams from the te\n",
    "trigram = gensim.models.Phrases(bigram[test_df[\"text_tokenized\"]], threshold=50)\n",
    "\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_model = gensim.models.phrases.Phraser(trigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define functions to remove stopwords, make bigrams, trigrams, then lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in spacy_stop_words] for doc in text]\n",
    "\n",
    "def make_bigrams(text):\n",
    "    return [bigram_model[doc] for doc in text]\n",
    "\n",
    "def make_trigrams(text):\n",
    "    return [trigram_model[bigram_mod[doc]] for doc in text]\n",
    "\n",
    "def lemmatization(text, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    text_out = []\n",
    "    for sent in text:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        text_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return text_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"text_no_stopwords\"] = remove_stopwords(test_df[\"text_tokenized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"text_bigrams\"] = make_bigrams(test_df[\"text_no_stopwords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 2094722 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-50578c1289ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_lemmatized\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_bigrams\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_postags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NOUN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ADJ'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'VERB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ADV'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-57ba0ffeaa9a>\u001b[0m in \u001b[0;36mlemmatization\u001b[0;34m(text, allowed_postags)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtext_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mtext_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_postags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             raise ValueError(\n\u001b[0;32m--> 428\u001b[0;31m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m             )\n\u001b[1;32m    430\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 2094722 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "test_df[\"text_lemmatized\"] = lemmatization(test_df[\"text_bigrams\"], allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
